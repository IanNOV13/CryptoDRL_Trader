import tensorflow as tf
import numpy as np
import os
import sys
import psutil
import signal
import datetime
from collections import deque
from dotenv import load_dotenv
import gc

# å°å…¥è‡ªå®šç¾©æ¨¡å¡Š
from config_loader import ConfigLoader # å‡è¨­ä½ å‰µå»ºäº†é€™å€‹æ¨¡å¡Š
from data_handler import prepare_data_and_env # å‡è¨­ä½ å‰µå»ºäº†é€™å€‹å‡½æ•¸ä¾†è™•ç†æ•¸æ“šå’Œå‰µå»ºç’°å¢ƒ
from DQNAgent import DQNAgent # æ ¸å¿ƒæ™ºèƒ½é«”
from evaluator import evaluate_model
from plotter import plot_segment
from utils import notify_discord_webhook, clear_lines, setup_gpu, set_mixed_precision_policy
save_on_exit_tag = False
load_dotenv()

# --- å»ºç«‹Ctrl+C æˆ–ç³»çµ±é€å‡ºçµ‚æ­¢è¨Šè™Ÿçš„ä¿å­˜å‹•ä½œ ---
def save_on_exit(signal_received, frame):
    global save_on_exit_tag
    save_on_exit_tag = not(save_on_exit_tag)
    notify_discord_webhook(f"ä¸­æ–·è¨Šè™Ÿæ›´æ”¹æˆ{save_on_exit_tag}ã€‚")
    

signal.signal(signal.SIGINT, save_on_exit)
signal.signal(signal.SIGTERM, save_on_exit)

def main():
    # 1. åŠ è¼‰é…ç½®
    config = ConfigLoader("setting.json").load_settings()
    setup_gpu() # GPU åˆå§‹åŒ–
    set_mixed_precision_policy() #å•Ÿç”¨æ··å’Œç²¾åº¦è¨“ç·´

    # 2. æº–å‚™æ•¸æ“šå’Œç’°å¢ƒ
    #    é€™å€‹å‡½æ•¸å…§éƒ¨å¯èƒ½è™•ç†æ•¸æ“šè·¯å¾‘ã€å‰µå»º CryptoTradingEnv å¯¦ä¾‹ã€è™•ç† Scaler
    train_env, test_env, scaler = prepare_data_and_env(config)

    # ç²å–ç’°å¢ƒä¿¡æ¯
    num_features = train_env.observation_space.shape[0]
    action_size = train_env.action_space.n
    model_input_shape = (config['SEQUENCE_LENGTH'], num_features)
    print(model_input_shape)
    # 3. åˆå§‹åŒ– Agent
    agent = DQNAgent(
        model_input_shape = model_input_shape,
        action_size = action_size,
        data = config
    )

    # 4. å˜—è©¦åŠ è¼‰æ¨¡å‹å’Œé€²åº¦ (æˆ–ç”± Agent å…§éƒ¨è™•ç†)
    start_episode = config["START_EPISODE"]
    best_reward = config["BEST_REWARD"]
    best_balance = config["BEST_BALANCE"]
    best_calmar_ratio = config["BEST_CALMAR_RATIO"]

    if os.path.exists(config["LOAD_MODEL_PATH"]):
        agent.load_model(config["LOAD_MODEL_PATH"])
        print("å¾æª¢æŸ¥é»ç¹¼çºŒè¨“ç·´...")
    else:
        print("é–‹å§‹æ–°çš„è¨“ç·´...")

    # 5. ä½¿ç”¨ä¸€å€‹ deque ä¾†é«˜æ•ˆåœ°ç¶­è­·ç‹€æ…‹åºåˆ—
    state_sequence_deque = deque(maxlen=config["SEQUENCE_LENGTH"])

    # 6. ä¸»è¨“ç·´å¾ªç’°
    global_step_count = config.get("global_step_count", 0)
    # --- ç²å–ç•¶å‰é€²ç¨‹å°è±¡ ---
    current_process = psutil.Process(os.getpid())

    for episode in range(start_episode, config["EPISODES"]):
        current_episode_experiences = []
        t = 0
        done = False
        new_calmar_ratio = False
        raw_state = train_env.reset()
        state_sequence_deque.clear()

        # --- åˆå§‹åŒ– state_sequence ---
        for _ in range(config["SEQUENCE_LENGTH"]):
            state_sequence_deque.append(raw_state)
        # --- åˆå§‹åŒ–çµæŸ ---

        # æ›´æ–° æ¢ç´¢ç‡ å’Œå­¸ç¿’ç‡
        current_epsilon = agent.get_current_epsilon(episode)
        current_lr = agent.get_current_lr(global_step_count)
        agent.set_optimizer_lr(current_lr)

        total_reward_episode = 0 # å–®å›åˆç¸½çå‹µçµ±è¨ˆ
        total_loss = 0
        loss = 0

        while not done:
            # --- ç²å–å‹•ä½œ ---
            # å°‡ deque è½‰æ›ç‚º NumPy æ•¸çµ„ï¼Œä½œç‚ºåˆå§‹çš„ç‹€æ…‹åºåˆ—
            state_sequence = np.array(state_sequence_deque)
            action = agent.select_action(state_sequence)
            next_state_sequence, reward_step, done, trade_executed = train_env.step(action)
            # --- ä¿å­˜ç•¶å‰ç¶“é©—ä¸¦æ›´æ–° ä¸¦æ›´æ–° state_sequence ---
            state_sequence_deque.append(next_state_sequence)
            next_state_sequence = np.array(state_sequence_deque)
            agent.store_experience(state_sequence, action, reward_step, next_state_sequence, done)
            current_episode_experiences.append((state_sequence, action, reward_step, next_state_sequence, done))
            
            total_reward_episode += reward_step
            state_sequence = next_state_sequence
            global_step_count += 1
            t += 1

            # è¨“ç·´ (æ¯éš”7æ­¥)
            if global_step_count % 7 == 0:
                if agent.can_train(): # agent å…§éƒ¨åˆ¤æ–·ç¶“é©—æ± å¤§å°ç­‰
                    loss = agent.train_batch() # å…§éƒ¨è™•ç†æ··åˆæŠ½æ¨£ã€DDQN Targetã€model.fit
                    total_loss = (total_loss + loss) / 2

                # æ›´æ–°ç›®æ¨™ç¶²çµ¡
                agent.update_target_model_if_needed(global_step_count)
            
            if t > 1:
                clear_lines(9)
            # ç²å–ç•¶å‰é€²ç¨‹ä½¿ç”¨çš„ç‰©ç†å…§å­˜ (RSS - Resident Set Size)
            mem_info = current_process.memory_info()
            rss_gb = mem_info.rss / (1024**3) # è½‰æ›ç‚º GB
            #é¡¯ç¤ºç›®å‰å›åˆçš„å³æ™‚è¨“ç·´æˆç¸¾
            print(f"ğŸš…{datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=8))).strftime('%Y-%m-%d %H:%M:%S')} ä¸­æ–·è¨Šè™Ÿï¼š{save_on_exit_tag}")
            print(f"å›åˆ{episode}/{config['EPISODES']}ï¼Œæ¢ç´¢æ©Ÿç‡ï¼š{current_epsilon:.4f}ï¼Œå­¸ç¿’ç‡ï¼š {current_lr:.6f}ï¼Œlossï¼š {loss:6.2f}/{total_loss:6.2f}")
            print(f"æ­¥æ•¸ï¼š{t}/{config['BEST_STEPS']}")
            print(f"ç¸½çå‹µï¼š{total_reward_episode:.2f}/{best_reward:6.2f}")
            print(f"äº¤æ˜“åˆ†æ•¸ï¼š{train_env.cumulative_trade_reward:6.2f}ï¼Œè³‡ç”¢è®ŠåŒ–åˆ†æ•¸ï¼š{train_env.cumulative_asset_change_reward:6.2f}ï¼ŒæŒæœ‰èˆ‡ä¸å‹•ä½œåˆ†æ•¸ï¼š{train_env.cumulative_hold_penalty:6.2f}ï¼Œå›å¾¹é€ç½°åˆ†æ•¸ï¼š{train_env.cumulative_drawdown_penalty:6.2f}ï¼Œç„¡æ•ˆäº¤æ˜“æ‡²ç½°ï¼š{train_env.cumulative_invalid_trade_penalty:6.2f}")
            print(f"é«˜è³£æ¬¡æ•¸ï¼š{train_env.hight_sell_timer}ï¼Œä½è³£æ¬¡æ•¸ï¼š{train_env.low_sell_timer}ï¼Œæ¯”ä¾‹{(train_env.hight_sell_timer/max(train_env.low_sell_timer,1)):6.2f}")
            print(f"RAMï¼š{rss_gb:.2f}/{config['MEMORY_LIMIT_GB']}")
            print(f"é¤˜é¡ï¼š{train_env.total_balance:6.2f}/{best_balance:6.2f}ï¼Œé¤˜é¡æ­·å²æœ€é«˜å€¼ï¼š{np.max(train_env.balance_history):6.2f}")
            print(f"æœ€å¤§å›æ’¤ï¼š{train_env.max_drawdown:6.2f}ï¼Œå¤æ™®æ¯”ç‡ï¼š{train_env.sharpe_ratio:6.2f}ï¼Œå¡çˆ¾é¦¬æ¯”ç‡ï¼š{train_env.calmar_ratio:6.2f}/{best_calmar_ratio:6.2f}")
        # --- å›åˆçµæŸ ---
        # --- æ›´æ–°æœ€ä½³ç¶“é©—æ±  ---
        best_reward, new_reward = agent.update_best_memory_if_needed(current_episode_experiences, total_reward_episode, best_reward, agent.best_reward_memory)
        best_balance, new_balance = agent.update_best_memory_if_needed(current_episode_experiences, train_env.total_balance, best_balance, agent.best_balance_memory)
        if train_env.calmar_ratio > best_calmar_ratio:
            best_calmar_ratio = train_env.calmar_ratio
            new_calmar_ratio = True
        # --- é¡¯ç¤ºæœ¬å›åˆæœ€çµ‚è³‡è¨Šï¼Œä¸¦å‚³é€åˆ°discord ---
        clear_lines(9)
        report_text = ""
        report_text += f"ğŸš©{datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=8))).strftime('%Y-%m-%d %H:%M:%S')}\n"
        report_text += f"å›åˆ{episode}/{config['EPISODES']}ï¼Œæ¢ç´¢æ©Ÿç‡ï¼š{current_epsilon:.4f}ï¼Œå­¸ç¿’ç‡: {current_lr:.6f}ï¼ŒAvg lossï¼š{total_loss:6.2f}\n"
        report_text += f"æ­¥æ•¸ï¼š{t}/{config['BEST_STEPS']}\n"
        report_text += f"ç¸½çå‹µï¼š{total_reward_episode:.2f}/{best_reward:6.2f}\n"
        report_text += f"äº¤æ˜“åˆ†æ•¸ï¼š{train_env.cumulative_trade_reward:6.2f}ï¼Œè³‡ç”¢è®ŠåŒ–åˆ†æ•¸ï¼š{train_env.cumulative_asset_change_reward:6.2f}ï¼ŒæŒæœ‰èˆ‡ä¸å‹•ä½œåˆ†æ•¸ï¼š{train_env.cumulative_hold_penalty:6.2f}ï¼Œå›å¾¹é€ç½°åˆ†æ•¸ï¼š{train_env.cumulative_drawdown_penalty:6.2f}ï¼Œç„¡æ•ˆäº¤æ˜“æ‡²ç½°ï¼š{train_env.cumulative_invalid_trade_penalty:6.2f}\n"
        report_text += f"é«˜è³£æ¬¡æ•¸ï¼š{train_env.hight_sell_timer}ï¼Œä½è³£æ¬¡æ•¸ï¼š{train_env.low_sell_timer}ï¼Œæ¯”ä¾‹{(train_env.hight_sell_timer/max(train_env.low_sell_timer,1)):6.2f}\n"
        report_text += f"RAMï¼š{rss_gb:.2f}/{config['MEMORY_LIMIT_GB']}\n"
        report_text += f"é¤˜é¡ï¼š{train_env.total_balance:6.2f}/{best_balance:6.2f}ï¼Œé¤˜é¡æ­·å²æœ€é«˜å€¼ï¼š{np.max(train_env.balance_history):6.2f}\n"
        report_text += f"æœ€å¤§å›æ’¤ï¼š{train_env.max_drawdown:6.2f}ï¼Œå¤æ™®æ¯”ç‡ï¼š{train_env.sharpe_ratio:6.2f}ï¼Œå¡çˆ¾é¦¬æ¯”ç‡ï¼š{train_env.calmar_ratio:6.2f}/{best_calmar_ratio:6.2f}\n"
        print(report_text)
        notify_discord_webhook(report_text)
        # --- ç¹ªåœ–èˆ‡ä¿å­˜æ¨¡å‹ã€é€²åº¦ ---
        if new_balance:
            agent.save_model(config["MODEL_PATH"] + "_BEST_BALANCE")
            notify_discord_webhook("GET NEW BEST BALANCE ğŸ‰")
        if new_reward:
            agent.save_model(config["MODEL_PATH"] + "_BEST_REWARD")
            notify_discord_webhook("GET NEW BEST REWARD ğŸ‰")
        if new_calmar_ratio:
            agent.save_model(config["MODEL_PATH"] + "_BEST_CALMAR_RATIO")
            notify_discord_webhook("GET NEW BEST CALMAR RATIO ğŸ‰")
        if episode%50 == 0:
            agent.save_model(config["MODEL_PATH"])
        
        if new_balance or new_reward or new_calmar_ratio or episode%25 == 0 or save_on_exit_tag:
            plot_segment(episode, train_env)
            evaluate_model(agent, test_env, model_input_shape[0], episode)
            ConfigLoader("setting.json").save_progress(
                episode = episode,
                best_reward = best_reward,
                best_balance = best_balance,
                best_calmar_ratio = best_calmar_ratio,
                current_epsilon = current_epsilon,
                current_lr = current_lr,
                global_step_count = global_step_count
            )

            if save_on_exit_tag:
                agent.save_model(config["MODEL_PATH"])
                notify_discord_webhook("æ¨¡å‹èˆ‡è¨“ç·´é€²åº¦å·²ä¿å­˜å®Œæˆã€‚")
                sys.exit(0)
        # --- è¨˜æ†¶é«”æ¸…ç† ---
        if rss_gb > config['MEMORY_LIMIT_GB']:
            agent.save_model(config["MODEL_PATH"])
            tf.keras.backend.clear_session()
            gc.collect()
            agent.load_model(config["MODEL_PATH"])
    # --- è¨“ç·´çµæŸ ---
    agent.save_model(os.path.join(config["MODEL_PATH"], "final_model"))
    print("è¨“ç·´å®Œæˆã€‚")

if __name__ == "__main__":
    main()