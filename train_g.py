import numpy as np
import random
import tensorflow as tf
from tensorflow.keras import mixed_precision # type: ignore
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import os
from collections import deque
from trading_env import CryptoTradingEnv # å‡è¨­é€™å€‹ class è£¡æœ‰ self.df å’Œ self.data (ç”¨æ–¼ state)
from dqn_gru_model import build_inception_gru_dqn_model
import gc
import pandas as pd # å¼•å…¥ pandas æ–¹ä¾¿è™•ç† DataFrame
from dotenv import load_dotenv
from linebot.v3.messaging import (
    ApiClient, # API å®¢æˆ¶ç«¯
    Configuration, # é…ç½®å°è±¡
    MessagingApi, # ä¸»è¦çš„ API æ¥å£
    PushMessageRequest, # æ¨é€æ¶ˆæ¯çš„è«‹æ±‚é«”
    TextMessage # æ–‡æœ¬æ¶ˆæ¯å°è±¡ (æ³¨æ„è·¯å¾‘ä¸åŒäº†)
)
import os
import psutil
import requests
import joblib
import datetime
import signal
import sys
import json

# --- è¼‰å…¥è¨­å®šæª” ---
with open("setting.json", "r") as f:
    setting = json.load(f)

# --- åƒæ•¸è¨­å®š ---
LOAD_MODEL_PATH = setting["LOAD_MODEL_PATH"]
MODEL_PATH = setting["MODEL_PATH"] #æ¨¡å‹åç¨±
SCALER_PATH = os.path.join(MODEL_PATH, "scaler.joblib")
EPISODES = setting["EPISODES"]  #ç¸½è¨“ç·´å›åˆ
BATCH_SIZE = setting["BATCH_SIZE"]
GAMMA = setting["GAMMA"]
INITIAL_EPSILON = setting["INITIAL_EPSILON"]  # æ¢ç´¢ç‡é€±æœŸçš„èµ·å§‹å€¼
MIN_EPSILON_CYCLE = setting["MIN_EPSILON_CYCLE"] # æ¢ç´¢ç‡åœ¨é€±æœŸçµæŸæ™‚çš„æœ€å°å€¼ (ç•¥é«˜æ–¼å…¨å±€ EPSILON_MIN)
EPSILON_CYCLE_LENGTH = setting["EPSILON_CYCLE_LENGTH"] # å¤šå°‘å€‹å›åˆä¸€å€‹æ¢ç´¢é€±æœŸ (å¯ä»¥åŸºæ–¼å›åˆæ•¸)
EPSILON_MIN = setting["EPSILON_MIN"] #æœ€å°æ¢ç´¢ç‡
MEMORY_SIZE = setting["MEMORY_SIZE"] # ç¶“é©—æ± å¤§å°å¯ä»¥å¤§ä¸€äº›ï¼Œå¢åŠ æ¨£æœ¬å¤šæ¨£æ€§
INITIAL_LR = setting["INITIAL_LR"] # åˆå§‹å­¸ç¿’ç‡ (å»ºè­° RL ä¸­ç”¨ç¨å°çš„å€¼é–‹å§‹)0.0001
LR_DROP_RATE = setting["LR_DROP_RATE"] # æœ€çµ‚å­¸ç¿’ç‡æ¯”ä¾‹
EPOCHS_DROP_RL = setting["EPOCHS_DROP_RL"] # é€€ç«é€±æœŸçš„ç¸½è¨“ç·´æ­¥æ•¸ (éœ€è¦ä¼°ç®—æˆ–è¨­å®š)
ALPHA_LR = setting["ALPHA_LR"]      # æœ€çµ‚æœ€å°å­¸ç¿’ç‡
TIME_RANGE = setting["TIME_RANGE"] # é è¨­ç¹ªåœ–é¡¯ç¤ºçš„æ™‚é–“çª—å£
NUM_PLOTS = setting["NUM_PLOTS"]     # çª—å£åˆ†å‰²æˆå¹¾å¼µåœ–ç‰‡
START_EPISODE = setting["START_EPISODE"]
BEST_STEPS = setting["BEST_STEPS"]
BEST_REWARD = setting["BEST_REWARD"] if setting["BEST_REWARD"] != None else -np.inf
BEST_BALANCE = setting["BEST_BALANCE"] if setting["BEST_BALANCE"] != None else -np.inf
SEQUENCE_LENGTH = setting["SEQUENCE_LENGTH"] #æ™‚é–“æ­¥é•·
MEMORY_LIMIT_GB = setting["MEMORY_LIMIT_GB"]
USE_LINE_BOT = setting["USE_LINE_BOT"]

# å¿½ç•¥ INFO å’Œ WARNING
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# --- GPU è¨­å®š ---
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)
print('Mixed precision policy set to:', mixed_precision.global_policy())

# --- å»ºç«‹Ctrl+C æˆ–ç³»çµ±é€å‡ºçµ‚æ­¢è¨Šè™Ÿçš„ä¿å­˜å‹•ä½œ ---
def save_on_exit(signal_received, frame):
    print("ä¸­æ–·è¨Šè™Ÿæ•æ‰åˆ°ï¼Œå„²å­˜æ¨¡å‹èˆ‡è¨“ç·´é€²åº¦ä¸­...")
    model.save(MODEL_PATH, save_format='tf')
    target_model.save(MODEL_PATH + "_target", save_format='tf')
    # å„²å­˜è¨“ç·´é€²åº¦
    progress = {
        "START_EPISODE": episode + 1,
        "BEST_REWARD": float(BEST_REWARD),
        "BEST_BALANCE": float(BEST_BALANCE),
        "EPSILON": float(EPSILON), # ç•¶å‰çš„ Epsilon
        "BEST_STEPS": BEST_STEPS,
        "current_lr": float(current_lr) if 'current_lr' in globals() and current_lr is not None else INITIAL_LR, # ç•¶å‰å­¸ç¿’ç‡
        "global_step_count": global_step_count if 'global_step_count' in globals() else 0
    }

    # æ‡‰è©²æ˜¯æ›´æ–° setting å­—å…¸ï¼Œç„¶å¾Œå¯«å›æ•´å€‹ setting å­—å…¸
    existing_settings = {}
    try:
        with open("setting.json", "r") as f:
            existing_settings = json.load(f)
    except FileNotFoundError:
        print("setting.json æœªæ‰¾åˆ°ï¼Œå°‡å‰µå»ºæ–°çš„ã€‚")
    except json.JSONDecodeError:
        print("setting.json æ ¼å¼éŒ¯èª¤ï¼Œå°‡ä½¿ç”¨æ–°çš„è¨­å®šè¦†è“‹ã€‚")

    existing_settings.update(progress) # ç”¨æ–°çš„é€²åº¦æ›´æ–°ï¼ˆæˆ–æ·»åŠ ï¼‰åˆ°å·²æœ‰çš„è¨­ç½®ä¸­

    with open("setting.json", "w") as f:
        json.dump(existing_settings, f, indent=4) # å¯«å›æ›´æ–°å¾Œçš„å®Œæ•´è¨­ç½®

    notify_discord_webhook("ä¸­æ–·è¨Šè™Ÿæ•æ‰åˆ°ï¼Œå·²å„²å­˜æ¨¡å‹èˆ‡è¨“ç·´é€²åº¦ã€‚å†è¦‹ ğŸ‘‹")
    sys.exit(0)

signal.signal(signal.SIGINT, save_on_exit)
signal.signal(signal.SIGTERM, save_on_exit)

# --- å‰µå»ºç’°å¢ƒ ---
env = CryptoTradingEnv("technical/BTCUSDT_1d_technical.csv",data_split="train")
test_env = CryptoTradingEnv("technical/BTCUSDT_1d_technical.csv",data_split="test")

# --- æ“¬åˆæˆ–åŠ è¼‰ Scaler ---
scaler_loaded_successfully = False # æ¨™èªŒä½ï¼Œåˆ¤æ–·æ˜¯å¦æˆåŠŸåŠ è¼‰

# æ­¥é©Ÿ 1: å˜—è©¦åŠ è¼‰
if os.path.exists(SCALER_PATH): # å…ˆæª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    try:
        loaded_scaler = joblib.load(SCALER_PATH)
        print(f"Scaler åŠ è¼‰è‡ª: {SCALER_PATH}")

        # æ­¥é©Ÿ 2: å¦‚æœåŠ è¼‰æˆåŠŸï¼Œæ‡‰ç”¨åˆ°ç’°å¢ƒ
        env.scaler = loaded_scaler
        test_env.scaler = loaded_scaler # æ‡‰ç”¨åŒä¸€å€‹ scaler
        env._scaler_fitted = True
        test_env._scaler_fitted = True
        scaler_loaded_successfully = True
        print("Scaler å·²æˆåŠŸåŠ è¼‰ä¸¦æ‡‰ç”¨æ–¼ç’°å¢ƒã€‚")

    except Exception as e:
        print(f"åŠ è¼‰ Scaler å¤±æ•— ({SCALER_PATH}): {e}. å°‡é‡æ–°æ”¶é›†æ•¸æ“šä¸¦æ“¬åˆã€‚")
        scaler_loaded_successfully = False # ç¢ºä¿æ¨™èªŒä½ç‚º False
else:
    print(f"Scaler æ–‡ä»¶æœªæ‰¾åˆ°: {SCALER_PATH}. å°‡æ”¶é›†æ•¸æ“šä¸¦æ“¬åˆã€‚")
    scaler_loaded_successfully = False

# æ­¥é©Ÿ 3: å¦‚æœåŠ è¼‰å¤±æ•—ï¼Œå‰‡åŸ·è¡Œæ•¸æ“šæ”¶é›†ã€æ“¬åˆå’Œä¿å­˜
if not scaler_loaded_successfully:
    print("é–‹å§‹æ”¶é›†ç‹€æ…‹æ•¸æ“šç”¨æ–¼æ“¬åˆ Scaler...")
    initial_states_for_scaler = []
    temp_env_for_scaler = None # åˆå§‹åŒ–ï¼Œä»¥ä¾¿åœ¨ finally ä¸­å®‰å…¨ä½¿ç”¨
    scaler_data = None       # åˆå§‹åŒ–

    try: # å°‡æ”¶é›†å’Œæ“¬åˆéç¨‹æ”¾åœ¨ try å¡Šä¸­ï¼Œä»¥ä¾¿ä½¿ç”¨ finally æ¸…ç†
        temp_env_for_scaler = CryptoTradingEnv("technical/BTCUSDT_1d_technical.csv", data_split="train")
        temp_state_unscaled = temp_env_for_scaler.get_current_unscaled_state_vector()
        initial_states_for_scaler.append(temp_state_unscaled)

        done = False
        step_count = 0
        max_collect_steps = len(temp_env_for_scaler.data) - 1
        print(f"å°‡åœ¨æœ€å¤š {max_collect_steps} æ­¥å…§æ”¶é›†ç‹€æ…‹...")

        while not done and step_count < max_collect_steps:
            action = temp_env_for_scaler.action_space.sample()
            _, _, done, _ = temp_env_for_scaler.step(action)
            next_state_unscaled = temp_env_for_scaler.get_current_unscaled_state_vector()
            initial_states_for_scaler.append(next_state_unscaled)
            step_count += 1
            # if step_count % 500 == 0: # å¯ä»¥å–æ¶ˆè¨»é‡‹ä¾†æŸ¥çœ‹é€²åº¦
            #     print(f"å·²æ”¶é›† {step_count} å€‹ç‹€æ…‹...")

        print(f"ç¸½å…±æ”¶é›†äº† {len(initial_states_for_scaler)} å€‹ç‹€æ…‹ç”¨æ–¼æ“¬åˆ Scalerã€‚")
        scaler_data = np.array(initial_states_for_scaler)

        # æ­¥é©Ÿ 4: æ“¬åˆ Scaler
        if scaler_data.shape[0] > 0 and scaler_data.shape[1] == env.observation_space.shape[0]:
            print("æ­£åœ¨æ“¬åˆ Scaler...")
            env.fit_scaler(scaler_data) # <<< æ“¬åˆ env çš„ scaler

            # æ­¥é©Ÿ 5: æ‡‰ç”¨åˆ°æ¸¬è©¦ç’°å¢ƒ
            print("å°‡æ“¬åˆå¥½çš„ Scaler æ‡‰ç”¨åˆ°æ¸¬è©¦ç’°å¢ƒ...")
            test_env.scaler = env.scaler # <<< å°‡æ“¬åˆå¥½çš„ env.scaler è¤‡è£½çµ¦ test_env
            test_env._scaler_fitted = True
            print("Scaler æ“¬åˆå®Œæˆä¸¦å·²æ‡‰ç”¨æ–¼è¨“ç·´å’Œæ¸¬è©¦ç’°å¢ƒã€‚")

            # æ­¥é©Ÿ 6: ä¿å­˜æ“¬åˆå¥½çš„ Scaler
            # ç¢ºä¿æ¨¡å‹ç›®éŒ„å­˜åœ¨
            if not os.path.exists(MODEL_PATH):
                os.makedirs(MODEL_PATH)
                print(f"å‰µå»ºç›®éŒ„: {MODEL_PATH}")
            try:
                joblib.dump(env.scaler, SCALER_PATH) # <<< ä¿å­˜æ“¬åˆå¥½çš„ env.scaler
                print(f"Scaler å·²ä¿å­˜åˆ°: {SCALER_PATH}")
            except Exception as e:
                print(f"[éŒ¯èª¤] ä¿å­˜ Scaler å¤±æ•—: {e}")

        else:
            # å¦‚æœæ”¶é›†æ•¸æ“šå¤±æ•—æˆ–ç¶­åº¦ä¸å°
            print(f"[éŒ¯èª¤] æ”¶é›†åˆ°çš„æ•¸æ“šç¶­åº¦ä¸æ­£ç¢ºæˆ–æ²’æœ‰æ”¶é›†åˆ°æ•¸æ“šï¼Shape: {scaler_data.shape if scaler_data is not None else 'None'}, Expected columns: {env.observation_space.shape[0]}")
            exit() # é€€å‡ºç¨‹åºï¼Œå› ç‚ºç„¡æ³•é€²è¡Œæ­¸ä¸€åŒ–

    finally: # ç„¡è«– try ä¸­æ˜¯å¦å‡ºéŒ¯ï¼Œéƒ½åŸ·è¡Œæ¸…ç†
        # æ­¥é©Ÿ 7: æ¸…ç†è‡¨æ™‚æ•¸æ“š
        print("æ¸…ç†è‡¨æ™‚æ•¸æ“š...")
        if temp_env_for_scaler is not None:
            del temp_env_for_scaler
        # ä½¿ç”¨ 'in locals()' æª¢æŸ¥è®Šé‡æ˜¯å¦å­˜åœ¨ï¼Œé¿å…åœ¨ç•°å¸¸æƒ…æ³ä¸‹å‡ºéŒ¯
        if 'initial_states_for_scaler' in locals():
            del initial_states_for_scaler
        if 'scaler_data' in locals() and scaler_data is not None:
            del scaler_data
        gc.collect()
        print("è‡¨æ™‚æ•¸æ“šå·²æ¸…ç†ã€‚")


# --- ç²å–ç’°å¢ƒä¿¡æ¯ ---
state_shape = env.observation_space.shape
if len(state_shape) != 1:
     raise ValueError(f"é æœŸç‹€æ…‹å½¢ç‹€ç‚º 1D (num_features,)ï¼Œä½†å¾—åˆ°çš„çµæœç‚º {state_shape}")
num_features = state_shape[0]
action_size = env.action_space.n
print(f"å–®æ­¥é©Ÿå½¢ç‹€ï¼š{state_shape}ï¼Œç‰¹å¾µæ•¸é‡ï¼š{num_features}ï¼Œå‹•ä½œæ•¸é‡ï¼š{action_size}")
print(f"ä½¿ç”¨åºåˆ—é•·åº¦ï¼š{SEQUENCE_LENGTH}")

# --- ç¹ªè£½å‡½æ•¸ (é‡ç”¨ç¹ªåœ–é‚è¼¯) ---
def plot_segment(plot_part_num, start_idx, end_idx, title_suffix,close_prices_all,mode="train"):
    plt.figure(figsize=(15, 7)) # æ¯å€‹éƒ¨åˆ†éƒ½å‰µå»ºä¸€å€‹æ–°åœ–å½¢
    plot_indices_segment = date[mode][start_idx:end_idx]#np.arange(start_idx, end_idx)
    close_prices_segment = close_prices_all[start_idx:end_idx]

    plt.plot(plot_indices_segment, close_prices_segment, label=f'{PRICE_COLUMN} Price', color='blue', alpha=0.8)

    # éæ¿¾è²·è³£é»åˆ°ç•¶å‰æ®µ (buy_positions å’Œ sell_positions å­˜å„²çš„æ˜¯ç´¢å¼•)
    buy_indices_in_segment = [idx_tuple[1] for idx_tuple in buy_positions if start_idx <= idx_tuple[1] < end_idx]
    sell_indices_in_segment = [idx_tuple[1] for idx_tuple in sell_positions if start_idx <= idx_tuple[1] < end_idx]
    
    if len(buy_indices_in_segment) > 0:
        buy_prices_in_segment = close_prices_all[buy_indices_in_segment]
        # è²·å…¥é»çš„ x åº§æ¨™éœ€è¦å¾ date å­—å…¸ä¸­æ ¹æ“šç´¢å¼•ç²å–
        buy_x_coords = [date[mode][i] for i in buy_indices_in_segment] # <--- ä¿®æ­£é€™è£¡
        plt.scatter(buy_x_coords, buy_prices_in_segment, color='lime', marker='^', label='Buy', s=100, edgecolors='black')

    if len(sell_indices_in_segment) > 0:
        sell_prices_in_segment = close_prices_all[sell_indices_in_segment]
        # è³£å‡ºé»çš„ x åº§æ¨™éœ€è¦å¾ date å­—å…¸ä¸­æ ¹æ“šç´¢å¼•ç²å–
        sell_x_coords = [date[mode][i] for i in sell_indices_in_segment] # <--- ä¿®æ­£é€™è£¡
        plt.scatter(sell_x_coords, sell_prices_in_segment, color='red', marker='v', label='Sell', s=100, edgecolors='black')

    # ç¹ªè£½è²·å…¥æ¨™è¨˜ä¸¦é¡¯ç¤ºè²·å…¥æ¯”ä¾‹
    for (action_ratio, idx) in buy_positions:
        if start_idx <= idx < end_idx:
            # æ–‡æœ¬æ¨™è¨˜çš„ x åº§æ¨™ä¹Ÿéœ€è¦å¾ date å­—å…¸ä¸­ç²å–
            text_x_coord = date[mode][idx] # <--- ä¿®æ­£é€™è£¡
            plt.text(text_x_coord, close_prices_all[idx] * 1.01, f"{action_ratio}", fontsize=9, color="green", ha="center")
    # ç¹ªè£½è³£å‡ºæ¨™è¨˜ä¸¦é¡¯ç¤ºè³£å‡ºæ¯”ä¾‹
    for (action_ratio, idx) in sell_positions:
        if start_idx <= idx < end_idx:
            # æ–‡æœ¬æ¨™è¨˜çš„ x åº§æ¨™ä¹Ÿéœ€è¦å¾ date å­—å…¸ä¸­ç²å–
            text_x_coord = date[mode][idx] # <--- ä¿®æ­£é€™è£¡
            plt.text(text_x_coord, close_prices_all[idx] * 0.99, f"{action_ratio}", fontsize=9, color="red", ha="center")

    plt.title(f'Episode {episode+1} - Trading Actions {title_suffix}')
    plt.xlabel('Date')
    # è‡ªå‹•æ ¼å¼åŒ–æ—¥æœŸæ¨™ç±¤ï¼Œä½¿å…¶æ›´ç¾è§€
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d')) # ä¾‹å¦‚ YYYY-MM-DD
    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator()) # è‡ªå‹•é¸æ“‡åˆé©çš„æ—¥æœŸé–“éš”
    plt.xticks(rotation=45) # æ—‹è½‰æ—¥æœŸæ¨™ç±¤ä»¥é˜²é‡ç–Š
    plt.ylabel('Price')
    plt.legend(loc='upper left')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    # å„²å­˜åœ–å½¢ï¼Œæ–‡ä»¶ååŒ…å«éƒ¨åˆ†ç·¨è™Ÿ
    # --- å„²å­˜ç›®éŒ„ ---
    save_dir = f'.//training_plots//episode_{episode+1}'
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    plot_filename_segment = os.path.join(save_dir, f"{mode}_episode_{episode+1}_part{plot_part_num}.png")
    plt.savefig(plot_filename_segment)
    plt.close() # é—œé–‰ç•¶å‰åœ–å½¢ï¼Œé‡‹æ”¾å…§å­˜
    notify_discord_webhook(plot_filename_segment,"image")

# --- æ¸…é™¤çµ‚ç«¯æ©Ÿè³‡è¨Š ---
def clear_lines(n):
    for _ in range(n):
        print("\033[F\033[K", end="")  # ä¸Šç§»ä¸€è¡Œ + æ¸…é™¤è©²è¡Œ

# --- åƒ¹æ ¼æ•¸æ“šç”¨æ–¼ç¹ªåœ– ---
PRICE_COLUMN = 'close'
if not hasattr(env, 'df') or not isinstance(env.df, pd.DataFrame):
     raise AttributeError("ç’°å¢ƒç‰©ä»¶â€œenvâ€å¿…é ˆå…·æœ‰ä¸€å€‹å±¬æ€§â€œdfâ€ï¼Œå®ƒæ˜¯ä¸€å€‹ Pandas DataFrameã€‚")
if PRICE_COLUMN not in env.df.columns:
     raise ValueError(f"env.df ä¸­æœªæ‰¾åˆ°åƒ¹æ ¼åˆ—â€œ{PRICE_COLUMN}â€")
close_prices_train = env.df[PRICE_COLUMN].values # ç²å–æ‰€æœ‰æ”¶ç›¤åƒ¹ç”¨æ–¼ç¹ªåœ–
close_prices_test = test_env.df[PRICE_COLUMN].values # ç²å–æ‰€æœ‰æ”¶ç›¤åƒ¹ç”¨æ–¼ç¹ªåœ–
date = {"train" : pd.to_datetime(env.df["timestamp"].values).to_list(), "test" : pd.to_datetime(test_env.df["timestamp"].values).to_list()}

# --- åˆå§‹åŒ– LINE Bot v3 API ---
# å‰µå»ºé…ç½®å°è±¡ï¼Œä¸¦è¨­ç½®ä½ çš„ Channel Access Token
configuration = Configuration(access_token=os.getenv('LINE_CHANNEL_ACCESS_TOKEN'))

# å»ºç«‹discord webhookä½œç‚ºline botçš„å‚™é¸æ–¹æ¡ˆ
def notify_discord_webhook(msg, send_type="text"):
    try:
        url = os.getenv("DISCORD_WEBHOOK")
        if send_type == "text":
            headers = {"Content-Type": "application/json"}
            data = {"content": msg, "username": "è¨“ç·´å›å ±"}
            res = requests.post(url, headers = headers, json = data) 
        elif send_type == "image":
            #headers = {"Content-Type": "multipart/form-data"}
            with open(msg, 'rb') as f:
                files = {
                    'file': (msg, f)
                }
                data = {"content": "", "username": "è¨“ç·´å›å ±"}
                res = requests.post(url, data = data, files = files) 
    except:
        pass

# å‰µå»º API å®¢æˆ¶ç«¯å¯¦ä¾‹
api_client = ApiClient(configuration)

# å‰µå»º MessagingApi å¯¦ä¾‹ï¼Œå‚³å…¥ API å®¢æˆ¶ç«¯
line_bot_api_v3 = MessagingApi(api_client) # å¯ä»¥å–ä¸€å€‹æ–°åå­—å€åˆ†

# --- ä½¿ç”¨ v3 API æ¨é€æ¶ˆæ¯ ---
try:
    if USE_LINE_BOT:
        # å‰µå»º PushMessageRequest å°è±¡
        push_message_request = PushMessageRequest(
            to=os.getenv('USER_ID'), # æŒ‡å®šæ¥æ”¶è€… USER_ID
            messages=[TextMessage(text="é–‹å§‹è¨“ç·´!!!")] # å‰µå»º TextMessage å°è±¡åˆ—è¡¨ (å³ä½¿åªæœ‰ä¸€æ¢æ¶ˆæ¯ä¹Ÿè¦æ˜¯åˆ—è¡¨)
        )
        # èª¿ç”¨æ–°çš„ API å¯¦ä¾‹çš„ push_message æ–¹æ³•
        line_bot_api_v3.push_message(push_message_request)
    else:
        print(f"use discord")
        notify_discord_webhook("é–‹å§‹è¨“ç·´!!!")
except Exception as e:
    print(f"Error sending LINE notification change to discord") # æ•ç²ä¸¦æ‰“å°å¯èƒ½çš„éŒ¯èª¤
    notify_discord_webhook("é–‹å§‹è¨“ç·´!!!")

# --- æ¨¡å‹è¼¸å…¥å½¢ç‹€ ---
MODEL_INPUT_SHAPE = (SEQUENCE_LENGTH, num_features)

# --- åŠ è¼‰æˆ–å‰µå»ºæ¨¡å‹ ---
try:
    model = tf.keras.models.load_model(LOAD_MODEL_PATH, compile=False)
    # ... compile model ...
    print("ç¹¼çºŒè¨“ç·´æ¨¡å‹!")
    # å¦‚æœæ˜¯ç¹¼çºŒè¨“ç·´ï¼Œæœ€å¥½ä¹ŸåŠ è¼‰ target_model çš„æ¬Šé‡ï¼ˆå¦‚æœä¹‹å‰æœ‰ä¿å­˜ï¼‰
    try:
        target_model = tf.keras.models.load_model(MODEL_PATH + "_target", compile=False) # å‡è¨­ä¿å­˜ç‚º _target å¾Œç¶´
        print("ç›®æ¨™ç¶²çµ¡å·²åŠ è¼‰ï¼")
    except:
        print("ç„¡æ³•åŠ è¼‰ç›®æ¨™ç¶²çµ¡ï¼Œå°‡å¾ä¸»ç¶²çµ¡è¤‡è£½æ¬Šé‡ã€‚")
        target_model = build_inception_gru_dqn_model(input_shape=MODEL_INPUT_SHAPE, action_size=action_size)
        target_model.set_weights(model.get_weights()) # åˆå§‹åŒ–æ™‚æ¬Šé‡ä¸€è‡´
except Exception as e:
    print(f"ç„¡æ³•åŠ è¼‰æ¨¡å‹ ({e}), é–‹å§‹è¨“ç·´æ–°æ¨¡å‹!")
    model = build_inception_gru_dqn_model(input_shape=MODEL_INPUT_SHAPE, action_size=action_size)
    # ... compile model ...
    target_model = build_inception_gru_dqn_model(input_shape=MODEL_INPUT_SHAPE, action_size=action_size)
    target_model.set_weights(model.get_weights()) # åˆå§‹åŒ–æ™‚æ¬Šé‡ä¸€è‡´
finally:
    optimizer = tf.keras.optimizers.Adam(learning_rate=INITIAL_LR) # ä½¿ç”¨åˆå§‹å­¸ç¿’ç‡
    model.compile(optimizer=optimizer, loss='mse')

# å‰µå»ºä¸€å€‹ç·¨è­¯å¾Œçš„é æ¸¬å‡½æ•¸
# --- ç·¨è­¯å¾Œçš„é æ¸¬å‡½æ•¸ç°½å ---
@tf.function(input_signature=[tf.TensorSpec(shape=(1, SEQUENCE_LENGTH, num_features), dtype=tf.float32)])
def compiled_predict(state_input):
  q_values = model(state_input, training=False)
  return q_values

@tf.function(input_signature=[tf.TensorSpec(shape=(None, SEQUENCE_LENGTH, num_features), dtype=tf.float32)])
def compiled_batch_predict(state_batch_input):
    """ä½¿ç”¨ tf.function ç·¨è­¯æ¨¡å‹çš„æ‰¹æ¬¡é æ¸¬"""
    # ç›´æ¥èª¿ç”¨æ¨¡å‹ï¼Œè¨­ç½® training=False
    q_values = model(state_batch_input, training=False)
    return q_values

@tf.function(input_signature=[tf.TensorSpec(shape=(None, SEQUENCE_LENGTH, num_features), dtype=tf.float32)])
def compiled_batch_predict_target(state_batch_input):
    """ä½¿ç”¨ tf.function ç·¨è­¯ç›®æ¨™ç¶²çµ¡çš„æ‰¹æ¬¡é æ¸¬"""
    q_values = target_model(state_batch_input, training=False) # ä½¿ç”¨ target_model
    return q_values

# --- æ–°å¢ï¼šè©•ä¼°å‡½æ•¸ ---
def evaluate_model(trained_model, eval_env, sequence_length, num_features, eval_episodes=1):
    """åœ¨æ¸¬è©¦ç’°å¢ƒä¸Šè©•ä¼°è¨“ç·´å¥½çš„æ¨¡å‹"""
    print(f"\n--- é–‹å§‹å°æ¸¬è©¦é›†é€²è¡Œè©•ä¼° ({eval_episodes} episodes)) ---")
    total_rewards = []
    final_balances = []
    max_drawdowns = []
    peak_balances = []
    all_hight_sells = []
    all_low_sells = []
    all_steps = []

    # å‰µå»ºè©•ä¼°ç”¨çš„é æ¸¬å‡½æ•¸ (ä½¿ç”¨å‚³å…¥çš„ trained_model)
    @tf.function(input_signature=[tf.TensorSpec(shape=(1, sequence_length, num_features), dtype=tf.float32)])
    def eval_predict(state_input):
        return trained_model(state_input, training=False)

    for i in range(eval_episodes):
        eval_raw_state = eval_env.reset()
        eval_state_deque = deque(maxlen=sequence_length)
        for _ in range(sequence_length):
            eval_state_deque.append(eval_raw_state)
        eval_state_sequence = np.array(eval_state_deque)

        eval_done = False
        eval_total_reward = 0
        eval_t = 0
        global buy_positions # <<< åˆå§‹åŒ– episode çš„è²·å…¥é»è¨˜éŒ„
        buy_positions = []
        global sell_positions # <<< åˆå§‹åŒ– episode çš„è³£å‡ºé»è¨˜éŒ„
        sell_positions = []
        
        while not eval_done:
            eval_t += 1
            eval_model_input = eval_state_sequence.reshape(1, sequence_length, num_features).astype(np.float32)
            # *** è©•ä¼°æ™‚é—œé–‰æ¢ç´¢ (Epsilon=0) ***
            eval_q_values_tensor = eval_predict(tf.convert_to_tensor(eval_model_input, dtype=tf.float32))
            eval_action = np.argmax(eval_q_values_tensor.numpy()[0])

            # *** æ³¨æ„ï¼šè©•ä¼°æ™‚ä¸æ‡‰æœ‰å‹•ä½œä¿®æ­£ï¼Œä»¥åæ˜ æ¨¡å‹çœŸå¯¦æ±ºç­– ***
            #if eval_env.balance <= 0 and (0 < eval_action < 6): eval_action = 0
            #elif eval_env.position <= 0 and (5 < eval_action < 11): eval_action = 0

            eval_next_raw_state, eval_reward, eval_done, eval_trade_executed = eval_env.step(eval_action)

            if eval_trade_executed:
                if 0 < eval_action < 6:
                    current_data_index = eval_env.current_step
                    buy_positions.append((eval_env.action_map[eval_action], current_data_index)) # ä½¿ç”¨ eval_action
                elif 5 < eval_action < 11 :
                    current_data_index = eval_env.current_step
                    sell_positions.append((eval_env.action_map[eval_action], current_data_index)) # ä½¿ç”¨ eval_action

            eval_state_deque.append(eval_next_raw_state)
            eval_next_state_sequence = np.array(eval_state_deque)

            eval_total_reward += eval_reward
            eval_state_sequence = eval_next_state_sequence

        data_len = len(close_prices_test)
        # å¦‚æœæ•¸æ“šä¸è¶³ TIME_RANGEï¼Œåªç¹ªè£½å¯ç”¨çš„æ•¸æ“š
        actual_range = data_len
        steps_per_plot = actual_range // 4
        remaining_steps = actual_range % 4 # è™•ç†é¤˜æ•¸æƒ…æ³

        # --- è¨ˆç®—ä¸¦ç¹ªè£½ç¬¬ä¸€éƒ¨åˆ† ---
        plot1_start_index = 0
        # ç¬¬ä¸€éƒ¨åˆ†åŒ…å«å‰åŠéƒ¨åˆ† + å¯èƒ½å¤šé¤˜çš„ä¸€æ­¥ (å¦‚æœ TIME_RANGE æ˜¯å¥‡æ•¸)
        plot1_end_index = steps_per_plot + remaining_steps
        if plot1_end_index > plot1_start_index: # ç¢ºä¿ç¯„åœæœ‰æ•ˆ
            plot_segment(1, plot1_start_index, plot1_end_index, f"(Steps {plot1_start_index}-{plot1_end_index-1})",close_prices_test,"test")
        # --- è¨ˆç®—ä¸¦ç¹ªè£½ç¬¬äºŒéƒ¨åˆ† ---
            plot2_start_index = plot1_end_index
        for i in range(actual_range // steps_per_plot - 2):
                plot2_end_index = plot1_end_index + (i+2) * steps_per_plot # ç¢ºä¿çµæŸé»æ­£ç¢º
                if plot2_end_index > plot2_start_index: # ç¢ºä¿ç¯„åœæœ‰æ•ˆ
                    plot_segment(i+2, plot2_start_index, plot2_end_index, f"(Steps {plot2_start_index}-{plot2_end_index-1})",close_prices_test,"test")
                plot2_start_index = plot2_end_index

        # è¨˜éŒ„è©•ä¼°çµæœ
        total_rewards.append(eval_total_reward)
        final_balances.append(eval_env.total_balance)
        max_drawdowns.append(eval_env.max_drawdown) # ç’°å¢ƒåœ¨ done æ™‚è¨ˆç®—
        peak_balances.append(np.max(eval_env.balance_history)) # å¾æ­·å²è¨˜éŒ„è¨ˆç®—å³°å€¼
        all_hight_sells.append(eval_env.hight_sell_timer)
        all_low_sells.append(eval_env.low_sell_timer)
        all_steps.append(eval_t)
        print(f"Eval Ep {i+1}: Steps={eval_t}, Reward={eval_total_reward:.2f}, Balance={eval_env.total_balance:.2f}, MaxDrawdown={eval_env.max_drawdown*100:.2f}%, Peak={np.max(eval_env.balance_history):.2f}")

    # è¨ˆç®—å¹³å‡çµæœ
    avg_reward = np.mean(total_rewards)
    avg_balance = np.mean(final_balances)
    avg_max_drawdown = np.mean(max_drawdowns)
    avg_peak = np.mean(peak_balances)
    print(f"--- Evaluation Summary (Avg over {eval_episodes} episodes) ---")
    print(f"Avg Reward: {avg_reward:.2f}")
    print(f"Avg Final Balance: {avg_balance:.2f}")
    print(f"Avg Max Drawdown: {avg_max_drawdown*100:.2f}%")
    print(f"Avg Peak Balance: {avg_peak:.2f}")
    print(f"----------------------------------------------------")
    return {"avg_reward": avg_reward, "avg_balance": avg_balance, "avg_max_drawdown": avg_max_drawdown}

# --- ç¶“é©—å›æ”¾æ±  ------
best_balance_memory = deque(maxlen=MEMORY_SIZE)
best_reward_memory = deque(maxlen=MEMORY_SIZE)
memory = deque(maxlen=MEMORY_SIZE)

# --- ç²å–ç•¶å‰é€²ç¨‹å°è±¡ ---
current_process = psutil.Process(os.getpid())

# --- è¨“ç·´éç¨‹ ---
global_step_count = setting.get("global_step_count", 0) # <--- éœ€è¦ä¸€å€‹å…¨å±€æ­¥æ•¸è¨ˆæ•¸å™¨
current_lr = 0

for episode in range(START_EPISODE,EPISODES):
    # --- ç‹€æ…‹åˆå§‹åŒ– ---
    raw_state = env.reset() # ç²å–ç¬¬ä¸€å€‹åŸå§‹ç‹€æ…‹ (num_features,)
    # ç²å–ç•¶å‰é€²ç¨‹ä½¿ç”¨çš„ç‰©ç†å…§å­˜ (RSS - Resident Set Size)
    mem_info = current_process.memory_info()
    rss_gb = mem_info.rss / (1024**3) # è½‰æ›ç‚º GB
    # ä½¿ç”¨ä¸€å€‹ deque ä¾†é«˜æ•ˆåœ°ç¶­è­·ç‹€æ…‹åºåˆ—
    state_sequence_deque = deque(maxlen=SEQUENCE_LENGTH)
    # ç”¨åˆå§‹ç‹€æ…‹å¡«å……éšŠåˆ— (é‡è¤‡ç¬¬ä¸€å€‹ç‹€æ…‹ SEQUENCE_LENGTH æ¬¡)
    for _ in range(SEQUENCE_LENGTH):
        state_sequence_deque.append(raw_state)
    # å°‡ deque è½‰æ›ç‚º NumPy æ•¸çµ„ï¼Œä½œç‚ºåˆå§‹çš„ç‹€æ…‹åºåˆ—
    state_sequence = np.array(state_sequence_deque) # Shape: (SEQUENCE_LENGTH, num_features)

    # --- è¨ˆç®—ç•¶å‰å›åˆçš„ Epsilon (åŸºæ–¼å›åˆæ•¸çš„é€±æœŸæ€§é€€ç«) ---
    progress_in_cycle = (episode % EPSILON_CYCLE_LENGTH) / EPSILON_CYCLE_LENGTH
    cosine_decay_epsilon = 0.5 * (1 + np.cos(np.pi * progress_in_cycle))
    current_epsilon_dynamic = (INITIAL_EPSILON - MIN_EPSILON_CYCLE) * cosine_decay_epsilon + MIN_EPSILON_CYCLE
    EPSILON = max(current_epsilon_dynamic, EPSILON_MIN) # ç¢ºä¿ä¸ä½æ–¼å…¨å±€æœ€å°å€¼

    total_reward = 0
    buy_positions = [] # <<< åˆå§‹åŒ– episode çš„è²·å…¥é»è¨˜éŒ„
    sell_positions = [] # <<< åˆå§‹åŒ– episode çš„è³£å‡ºé»è¨˜éŒ„
    current_episode_experiences = [] # <--- æš«å­˜ç•¶å‰å›åˆç¶“é©—
    done = False
    t = 0 # æ™‚é–“æ­¥è¨ˆæ•¸å™¨

    # --- ä½¿ç”¨æ¨™æº– RL å¾ªç’° ---
    while not done:
        t += 1
        global_step_count += 1 # æ›´æ–°å…¨å±€æ­¥æ•¸
        # --- ä¿®æ”¹ï¼šæº–å‚™æ¨¡å‹è¼¸å…¥ ---
        # å°‡ state_sequence reshape æˆæ¨¡å‹æœŸæœ›çš„è¼¸å…¥ (1, sequence_length, num_features)
        model_input_state = state_sequence.reshape(1, SEQUENCE_LENGTH, num_features).astype(np.float32)
        # æ¢ç´¢æˆ–åˆ©ç”¨ç­–ç•¥
        if np.random.rand() < EPSILON:
            action = np.random.choice(action_size)
        else:
            # ä½¿ç”¨ç·¨è­¯å¾Œçš„å‡½æ•¸é€²è¡Œé æ¸¬
            q_values_tensor = compiled_predict(tf.convert_to_tensor(model_input_state, dtype=tf.float32))
            action = np.argmax(q_values_tensor.numpy()[0])

        if env.balance <= 0 and (0 < action < 6):
            action = 0
        elif env.position <= 0 and (5 < action < 11):
            action = 0

        # åŸ·è¡Œé¸æ“‡çš„å‹•ä½œ (å¯èƒ½æ˜¯ä¿®æ”¹å¾Œçš„ action)
        next_state_raw, reward, done, trade_executed = env.step(action)

        # --- ä¿®æ”¹ï¼šæ›´æ–°ç‹€æ…‹åºåˆ— ---
        # å°‡æ–°çš„åŸå§‹ç‹€æ…‹æ·»åŠ åˆ° deque çš„æœ«å°¾ (æœ€èˆŠçš„æœƒè‡ªå‹•ç§»é™¤)
        state_sequence_deque.append(next_state_raw)
        # å°‡æ›´æ–°å¾Œçš„ deque è½‰æ›ç‚º NumPy æ•¸çµ„ï¼Œä½œç‚º next_state_sequence
        next_state_sequence = np.array(state_sequence_deque) # Shape: (SEQUENCE_LENGTH, num_features)
        
        # --- è¨˜éŒ„äº¤æ˜“è¡Œç‚º ---
        # ä½¿ç”¨åŸ·è¡Œçš„å‹•ä½œ action ä¾†æ›´æ–°ç‹€æ…‹å’Œè¨˜éŒ„ä½ç½®
        if trade_executed:
            if 0 < action < 6:  # å¯¦éš›åŸ·è¡Œäº†è²·å…¥
                # ä½¿ç”¨ env å…§éƒ¨çš„æ™‚é–“ç´¢å¼•æˆ–è€… t-1 (å› ç‚º t æ˜¯ç•¶å‰æ­¥æ•¸ï¼Œå‹•ä½œç™¼ç”Ÿåœ¨ t-1 çµæŸæ™‚)
                # å‡è¨­ env.current_step è¨˜éŒ„äº† DataFrame çš„ç´¢å¼•
                # å¦‚æœæ²’æœ‰ï¼Œç”¨ t-1 å¯èƒ½æœƒæœ‰åå·®ï¼Œå–æ±ºæ–¼ env å¦‚ä½•å¯¦ç¾
                current_data_index = env.current_step # å‡è¨­ç’°å¢ƒæœ‰é€™å€‹å±¬æ€§
                buy_positions.append((env.action_map[action],current_data_index)) # è¨˜éŒ„æ•¸æ“šçš„ç´¢å¼•
            elif 5 < action < 11 :  # å¯¦éš›åŸ·è¡Œäº†è³£å‡º
                current_data_index = env.current_step
                sell_positions.append((env.action_map[action],current_data_index))

        # å­˜å„²ç¶“é©— (åŸå§‹ state, å¯¦éš›åŸ·è¡Œçš„ action, reward, next_state, done)
        memory.append((state_sequence, action, reward, next_state_sequence, done))
        current_episode_experiences.append((state_sequence, action, reward, next_state_sequence, done))

        total_reward += reward
        state_sequence = next_state_sequence # *** æ›´æ–° state_sequence ä»¥é€²è¡Œä¸‹ä¸€æ­¥ ***

        # --- åœ¨æ¯ä¸€æ­¥å¾Œå˜—è©¦è¨“ç·´ ---
        if len(memory) >= BATCH_SIZE and (t-1)%7 == 0: # ç¢ºä¿ä¸‰å€‹æ± å­éƒ½æœ‰æ•¸æ“š
            # --- æ··åˆæŠ½æ¨£ ---
            can_mixed_sample = len(memory) >= BATCH_SIZE // 2 and (len(best_balance_memory) > 0 or len(best_reward_memory) > 0)

            if can_mixed_sample:
                # 1. è¨ˆç®—å„ä¾†æºçš„ç›®æ¨™æ¨£æœ¬æ•¸é‡ (ç¢ºä¿ç‚ºæ•´æ•¸ä¸”ç¸½å’Œç‚º BATCH_SIZE)
                main_target_size = int(BATCH_SIZE * 0.9)
                remaining_size = BATCH_SIZE - main_target_size

                # æŒ‰æ¯”ä¾‹åˆ†é…å‰©é¤˜åé¡ï¼Œè™•ç†å–æ•´
                balance_target_size = int(remaining_size * 0.8)
                # å°‡é¤˜æ•¸åˆ†é…çµ¦ reward æ± ï¼Œç¢ºä¿ç¸½æ•¸æ­£ç¢º
                reward_target_size = remaining_size - balance_target_size
            else:
                main_target_size = BATCH_SIZE

            # (å¯é¸) æª¢æŸ¥ä¸»æ± å¯¦éš›å¤§å°ï¼Œå¦‚æœä¸è¶³ main_target_sizeï¼Œéœ€è¦èª¿æ•´
            if len(memory) < main_target_size:
                print(f"[è­¦å‘Š] ä¸»ç¶“é©—æ± æ¨£æœ¬ä¸è¶³ {main_target_size}ï¼Œå¯¦éš›åªæœ‰ {len(memory)}")
                main_target_size = len(memory) # åªèƒ½æŠ½å–é€™éº¼å¤š
                # é‡æ–°è¨ˆç®—å‰©é¤˜çš„ï¼Œä¸¦é‡æ–°åˆ†é…çµ¦ best pools
                remaining_size = BATCH_SIZE - main_target_size
                balance_target_size = int(remaining_size * 0.8)
                reward_target_size = remaining_size - balance_target_size

            # 2. å¾ä¸»ç¶“é©—æ± æŠ½æ¨£
            main_minibatch = random.sample(memory, main_target_size)

            # 3. å¾ Best Balance æ± æŠ½æ¨£
            if len(best_balance_memory) > 0:
                # ç„¡è«–æ•¸é‡æ˜¯å¦è¶³å¤ ï¼Œéƒ½ä½¿ç”¨ random.choices (å…è¨±é‡è¤‡) æŠ½å–ç›®æ¨™æ•¸é‡
                balance_batch = random.choices(list(best_balance_memory), k=balance_target_size)
            else:
                balance_batch = [] # å¦‚æœæ± å­æ˜¯ç©ºçš„

            # 4. å¾ Best Reward æ± æŠ½æ¨£
            if len(best_reward_memory) > 0:
                reward_batch = random.choices(list(best_reward_memory), k=reward_target_size)
            else:
                reward_batch = [] # å¦‚æœæ± å­æ˜¯ç©ºçš„

            # 5. åˆä½µåˆæ­¥çš„ minibatch
            minibatch = main_minibatch + balance_batch + reward_batch

            # 6. è™•ç†å› æœ€ä½³æ± ç‚ºç©ºå°è‡´çš„æ¨£æœ¬ä¸è¶³æƒ…æ³
            current_size = len(minibatch)
            missing_count = BATCH_SIZE - current_size

            if missing_count > 0:
                print(f"æ··åˆæŠ½æ¨£å¾Œæ¨£æœ¬ä¸è¶³ï¼Œéœ€è¦å¾ä¸»æ± è£œå…… {missing_count} å€‹")
                # å¾ä¸»æ± è£œå……ç¼ºå°‘çš„æ¨£æœ¬
                if len(memory) >= main_target_size + missing_count: # æª¢æŸ¥ä¸»æ± æ˜¯å¦æœ‰è¶³å¤ çš„é¡å¤–æ¨£æœ¬
                    # å˜—è©¦æŠ½å–èˆ‡ main_minibatch ä¸é‡è¤‡çš„æ¨£æœ¬ (è¼ƒè¤‡é›œ)
                    # ç°¡åŒ–è™•ç†ï¼šç›´æ¥å†æŠ½ missing_count å€‹ï¼Œå…è¨±å°‘é‡é‡è¤‡
                    supplement_batch = random.sample(memory, missing_count)
                    minibatch.extend(supplement_batch)
                elif len(memory) > main_target_size: # å¦‚æœåªèƒ½è£œå……éƒ¨åˆ†
                    available_supplement = len(memory) - main_target_size
                    supplement_batch = random.sample(memory, available_supplement)
                    minibatch.extend(supplement_batch)
                    print(f"[è­¦å‘Š] ä¸»æ± åªèƒ½è£œå…… {available_supplement} å€‹ï¼Œæœ€çµ‚æ‰¹æ¬¡å¤§å°ç‚º {len(minibatch)}")
            
            # 7. æ‰“äº‚æœ€çµ‚çš„ minibatch é †åº
            random.shuffle(minibatch)
            actual_batch_size = len(minibatch)

            # --- è¨ˆç®—ç•¶å‰å­¸ç¿’ç‡ ---
            # ä½¿ç”¨ global_step_count ä½œç‚º cosine_decay çš„è¼¸å…¥
            cosine_decay = 0.5 * (1 + np.cos(np.pi * (global_step_count % EPOCHS_DROP_RL) / EPOCHS_DROP_RL))
            # è®“å­¸ç¿’ç‡å¾ INITIAL_LR é™åˆ° ALPHA_LR
            # new_lr = (INITIAL_LR - ALPHA_LR) * cosine_decay + ALPHA_LR
            # æˆ–è€…ä½¿ç”¨ä½ çš„ drop_rate æ¨¡å¼ï¼Œä½†çµ‚é»è¨­ç‚º ALPHA_LR
            decayed = (1 - LR_DROP_RATE) * cosine_decay + LR_DROP_RATE # é€™è£¡ drop_rate ä»£è¡¨æœ€ä½æ¯”ä¾‹
            new_lr = INITIAL_LR * decayed
            current_lr = max(new_lr, ALPHA_LR) # ç¢ºä¿ä¸ä½æ–¼æœ€å°å­¸ç¿’ç‡

            # --- æ›´æ–°å„ªåŒ–å™¨çš„å­¸ç¿’ç‡ ---
            tf.keras.backend.set_value(model.optimizer.learning_rate, current_lr)

            # --- æ‰¹æ¬¡è™•ç† ---
            # 0. ç²å–å¯¦éš›çš„æ‰¹æ¬¡å¤§å°
            actual_batch_size = len(minibatch)

            # 1. å¾ minibatch ä¸­è§£åŒ…æ•¸æ“š
            state_sequences_batch = np.array([transition[0] for transition in minibatch])
            actions_batch = np.array([transition[1] for transition in minibatch])
            rewards_batch = np.array([transition[2] for transition in minibatch])
            next_state_sequences_batch = np.array([transition[3] for transition in minibatch])
            dones_batch = np.array([transition[4] for transition in minibatch])

            # --- DDQN æ ¸å¿ƒè¨ˆç®— ---
            # 2. ä½¿ç”¨ä¸»ç¶²çµ¡ (model) é æ¸¬ç•¶å‰ç‹€æ…‹ (s) çš„ Q å€¼
            #    Q_online(s, a)
            q_values_current_tensor = compiled_batch_predict(
                tf.convert_to_tensor(state_sequences_batch, dtype=tf.float32)
            )
            q_values_current = q_values_current_tensor.numpy() # è½‰æ›ç‚º NumPy æ•¸çµ„

            # 3. ä½¿ç”¨ä¸»ç¶²çµ¡ (model) é æ¸¬ä¸‹ä¸€å€‹ç‹€æ…‹ (s') çš„ Q å€¼ï¼Œç”¨æ–¼é¸æ“‡å‹•ä½œ a_max
            #    Q_online(s', a)
            q_values_next_online_tensor = compiled_batch_predict(
                tf.convert_to_tensor(next_state_sequences_batch, dtype=tf.float32)
            )
            q_values_next_online = q_values_next_online_tensor.numpy() # è½‰æ›ç‚º NumPy æ•¸çµ„

            # 4. ä½¿ç”¨ç›®æ¨™ç¶²çµ¡ (target_model) é æ¸¬ä¸‹ä¸€å€‹ç‹€æ…‹ (s') çš„ Q å€¼ï¼Œç”¨æ–¼è©•ä¼°é¸å®šå‹•ä½œçš„åƒ¹å€¼
            #    Q_target(s', a)
            q_values_next_target_tensor = compiled_batch_predict_target( # ä½¿ç”¨ç›®æ¨™ç¶²çµ¡çš„é æ¸¬å‡½æ•¸
                tf.convert_to_tensor(next_state_sequences_batch, dtype=tf.float32)
            )
            q_values_next_target = q_values_next_target_tensor.numpy() # è½‰æ›ç‚º NumPy æ•¸çµ„
            # --- DDQN æ ¸å¿ƒè¨ˆç®—çµæŸ ---

            # 5. åˆå§‹åŒ– targets æ•¸çµ„ï¼Œå…¶åŸºç¤æ˜¯ç•¶å‰ç‹€æ…‹çš„ Q å€¼
            targets = np.copy(q_values_current)

            # 6. è¨ˆç®—ç›®æ¨™ Q å€¼ (TD Target) - ä½¿ç”¨ DDQN é‚è¼¯
            #    å¾ªç’°ä½¿ç”¨å¯¦éš›çš„æ‰¹æ¬¡å¤§å°
            for i in range(actual_batch_size):
                if dones_batch[i]:
                    # å¦‚æœæ˜¯çµ‚æ­¢ç‹€æ…‹ï¼Œç›®æ¨™ Q å€¼å°±æ˜¯å³æ™‚çå‹µ
                    targets[i, actions_batch[i]] = rewards_batch[i]
                else:
                    # DDQN æ­¥é©Ÿï¼š
                    # a_max = argmax_a' Q_online(s'[i], a')
                    # å¾ä¸»ç¶²çµ¡å°ä¸‹ä¸€å€‹ç‹€æ…‹çš„ Q å€¼é æ¸¬ä¸­ï¼Œæ‰¾åˆ°ä½¿ Q å€¼æœ€å¤§çš„å‹•ä½œçš„ç´¢å¼•
                    action_max_online = np.argmax(q_values_next_online[i])

                    # Target_DDQN = R[i] + Î³ * Q_target(s'[i], a_max)
                    # ä½¿ç”¨ç›®æ¨™ç¶²çµ¡å°ä¸‹ä¸€å€‹ç‹€æ…‹çš„ Q å€¼é æ¸¬ï¼Œä¾†ç²å– action_max_online å°æ‡‰çš„ Q å€¼
                    targets[i, actions_batch[i]] = rewards_batch[i] + GAMMA * q_values_next_target[i, action_max_online]

            # 7. æ‰¹æ¬¡è¨“ç·´ä¸»ç¶²çµ¡ (model)
            #    ä½¿ç”¨å¯¦éš›çš„æ‰¹æ¬¡å¤§å°
            model.fit(state_sequences_batch, targets, epochs=1, verbose=0, batch_size=actual_batch_size)

            if t > 1:
                clear_lines(8)  # æ¸…é™¤ä¸Šæ¬¡è¼¸å‡ºçš„ 10 è¡Œ
            #é¡¯ç¤ºç›®å‰å›åˆçš„å³æ™‚è¨“ç·´æˆç¸¾
            print(f"ğŸš…{datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=8))).strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"å›åˆ{episode+1}/{EPISODES}ï¼Œæ¢ç´¢æ©Ÿç‡ï¼š{EPSILON:.4f}ï¼Œå­¸ç¿’ç‡: {current_lr:.6f}")
            print(f"æ­¥æ•¸ï¼š{t+1}/{BEST_STEPS}")
            print(f"ç¸½çå‹µï¼š{total_reward:.2f}/{BEST_REWARD:6.2f}")
            print(f"äº¤æ˜“åˆ†æ•¸ï¼š{env.cumulative_trade_reward:6.2f}ï¼Œè³‡ç”¢è®ŠåŒ–åˆ†æ•¸ï¼š{env.cumulative_asset_change_reward:6.2f}ï¼ŒæŒæœ‰èˆ‡ä¸å‹•ä½œåˆ†æ•¸ï¼š{env.cumulative_hold_penalty:6.2f}ï¼Œå›å¾¹é€ç½°åˆ†æ•¸ï¼š{env.cumulative_drawdown_penalty:6.2f}ï¼Œç„¡æ•ˆäº¤æ˜“æ‡²ç½°ï¼š{env.cumulative_invalid_trade_penalty:6.2f}")
            print(f"é«˜è³£æ¬¡æ•¸ï¼š{env.hight_sell_timer}ï¼Œä½è³£æ¬¡æ•¸ï¼š{env.low_sell_timer}ï¼Œæ¯”ä¾‹{(env.hight_sell_timer/max(env.low_sell_timer,1)):6.2f}")
            print(f"RAMï¼š{rss_gb:.2f}/{MEMORY_LIMIT_GB}")
            print(f"é¤˜é¡ï¼š{env.total_balance:6.2f}/{BEST_BALANCE:6.2f}ï¼Œé¤˜é¡æ­·å²æœ€é«˜å€¼ï¼š{np.max(env.balance_history):6.2f}ï¼Œæœ€å¤§å›æ’¤ï¼š{env.max_drawdown:6.2f}")

    target_model.set_weights(model.get_weights())

    #é¡¯ç¤ºèˆ‡å‚³é€æ¯å›åˆçš„è¨“ç·´æˆç¸¾
    clear_lines(8)
    report_text = f"""ğŸš©{datetime.datetime.now(tz=datetime.timezone(datetime.timedelta(hours=8))).strftime('%Y-%m-%d %H:%M:%S')}
    å›åˆ{episode+1}/{EPISODES}ï¼Œæ¢ç´¢æ©Ÿç‡ï¼š{EPSILON:.4f}ï¼Œå­¸ç¿’ç‡: {current_lr:.6f}
    æ­¥æ•¸ï¼š{t}/{BEST_STEPS}
    ç¸½çå‹µï¼š{total_reward:.2f}/{BEST_REWARD:6.2f}
    äº¤æ˜“åˆ†æ•¸ï¼š{env.cumulative_trade_reward:6.2f}ï¼Œè³‡ç”¢è®ŠåŒ–åˆ†æ•¸ï¼š{env.cumulative_asset_change_reward:6.2f}ï¼ŒæŒæœ‰èˆ‡ä¸å‹•ä½œåˆ†æ•¸ï¼š{env.cumulative_hold_penalty:6.2f}ï¼Œå›å¾¹é€ç½°åˆ†æ•¸ï¼š{env.cumulative_drawdown_penalty:6.2f}ï¼Œç„¡æ•ˆäº¤æ˜“æ‡²ç½°ï¼š{env.cumulative_invalid_trade_penalty:6.2f}
    é«˜è³£æ¬¡æ•¸ï¼š{env.hight_sell_timer}ï¼Œä½è³£æ¬¡æ•¸ï¼š{env.low_sell_timer}ï¼Œæ¯”ä¾‹{(env.hight_sell_timer/max(env.low_sell_timer,1)):6.2f}
    RAMï¼š{rss_gb:.2f}/{MEMORY_LIMIT_GB}
    é¤˜é¡ï¼š{env.total_balance:6.2f}/{BEST_BALANCE:6.2f}ï¼Œé¤˜é¡æ­·å²æœ€é«˜å€¼ï¼š{np.max(env.balance_history):6.2f}ï¼Œæœ€å¤§å›æ’¤ï¼š{env.max_drawdown:6.2f}
    ğŸ”š
    """
    print(report_text)

    # --- Episode çµæŸ ---

    # --- ä½¿ç”¨ v3 API æ¨é€æ¶ˆæ¯ ---
    try:
        if USE_LINE_BOT:
            # å‰µå»º PushMessageRequest å°è±¡
            push_message_request = PushMessageRequest(
                to=os.getenv('USER_ID'), # æŒ‡å®šæ¥æ”¶è€… USER_ID
                messages=[TextMessage(text=report_text)] # å‰µå»º TextMessage å°è±¡åˆ—è¡¨ (å³ä½¿åªæœ‰ä¸€æ¢æ¶ˆæ¯ä¹Ÿè¦æ˜¯åˆ—è¡¨)
            )
            # èª¿ç”¨æ–°çš„ API å¯¦ä¾‹çš„ push_message æ–¹æ³•
            line_bot_api_v3.push_message(push_message_request)
        else:
            notify_discord_webhook(report_text)

    except:
         notify_discord_webhook(report_text)
    
    # --- ç¹ªåœ– (æ¯ 25 episodes) ---
    try:
        if episode % 25 == 0 or t > BEST_STEPS or total_reward > BEST_REWARD or env.total_balance > BEST_BALANCE:
            if BEST_STEPS > TIME_RANGE:
                TIME_RANGE = BEST_STEPS
                NUM_PLOTS = TIME_RANGE//100 
            data_len = len(close_prices_train)
            # å¦‚æœæ•¸æ“šä¸è¶³ TIME_RANGEï¼Œåªç¹ªè£½å¯ç”¨çš„æ•¸æ“š
            actual_range = min(TIME_RANGE, data_len)
            steps_per_plot = actual_range // NUM_PLOTS
            remaining_steps = actual_range % NUM_PLOTS # è™•ç†å¥‡æ•¸æƒ…æ³

            # --- è¨ˆç®—ä¸¦ç¹ªè£½ç¬¬ä¸€éƒ¨åˆ† ---
            plot1_start_index = 0
            # ç¬¬ä¸€éƒ¨åˆ†åŒ…å«å‰åŠéƒ¨åˆ† + å¯èƒ½å¤šé¤˜çš„ä¸€æ­¥ (å¦‚æœ TIME_RANGE æ˜¯å¥‡æ•¸)
            plot1_end_index = steps_per_plot + remaining_steps #250
            if plot1_end_index > plot1_start_index: # ç¢ºä¿ç¯„åœæœ‰æ•ˆ
                plot_segment(1, plot1_start_index, plot1_end_index, f"(Steps {plot1_start_index}-{plot1_end_index-1})",close_prices_train)
            # --- è¨ˆç®—ä¸¦ç¹ªè£½ç¬¬äºŒéƒ¨åˆ† ---
                plot2_start_index = plot1_end_index
            for i in range(NUM_PLOTS - 2):
                    plot2_end_index = plot1_end_index + (i+2) * steps_per_plot # ç¢ºä¿çµæŸé»æ­£ç¢º
                    if plot2_end_index > plot2_start_index: # ç¢ºä¿ç¯„åœæœ‰æ•ˆ
                        plot_segment(i+2, plot2_start_index, plot2_end_index, f"(Steps {plot2_start_index}-{plot2_end_index-1})",close_prices_train)
                    plot2_start_index = plot2_end_index
    except Exception as e:
        print(f"ç¹ªåœ–ç™¼ç”ŸéŒ¯èª¤:{e}")


    # --- å®šæœŸä¿å­˜æ¨¡å‹ ---
    if episode % 50 == 0 or t > BEST_STEPS or total_reward > BEST_REWARD or env.total_balance > BEST_BALANCE:
        print(f"Saving model at episode {episode+1}...")
        if t > BEST_STEPS:
            BEST_STEPS = t
            model.save((MODEL_PATH + "_BEST_STEP"), save_format='tf')
            notify_discord_webhook("GET NEW BEST STEP ğŸ‰")
        if total_reward > BEST_REWARD:
            BEST_REWARD = total_reward
            #best_reward_memory.clear() # æ¸…ç©ºèˆŠçš„æœ€ä½³å›åˆç¶“é©—
            for exp in current_episode_experiences: # å°‡æ–°æœ€ä½³å›åˆç¶“é©—å­˜å…¥
                best_reward_memory.append(exp)
            model.save((MODEL_PATH + "_BEST_REWARD"), save_format='tf')
            notify_discord_webhook("GET NEW BEST REWARD ğŸ‰")
        if env.total_balance > BEST_BALANCE:
            BEST_BALANCE = env.total_balance
            #best_balance_memory.clear() # æ¸…ç©ºèˆŠçš„æœ€ä½³å›åˆç¶“é©—
            for exp in current_episode_experiences: # å°‡æ–°æœ€ä½³å›åˆç¶“é©—å­˜å…¥
                best_balance_memory.append(exp)
            model.save((MODEL_PATH + "_BEST_BALANCE"), save_format='tf')
            notify_discord_webhook("GET NEW BEST BALANCE ğŸ‰")

        model.save(MODEL_PATH, save_format='tf')
        eval_results = evaluate_model(model, test_env, SEQUENCE_LENGTH, num_features)
        eval_report = f"""
        è©•ä¼°çµæœğŸ“ˆ - 
        å¹³å‡çå‹µï¼š{eval_results['avg_reward']:.2f}ï¼Œå¹³å‡é¤˜é¡ï¼š{eval_results['avg_balance']:.2f}ï¼Œå¹³å‡æœ€å¤§å›æ’¤ï¼š{eval_results['avg_max_drawdown']*100:.2f}%
        """
        notify_discord_webhook(eval_report)
        

    # --- å¯é¸çš„è¨˜æ†¶é«”æ¸…ç† ---
    if rss_gb > MEMORY_LIMIT_GB:
        del minibatch, actions_batch, rewards_batch, dones_batch
        del q_values_current, targets
        try:
            current_model_path = MODEL_PATH # æˆ–è€…ä¸€å€‹è‡¨æ™‚è·¯å¾‘
            model.save(current_model_path) # ä¿å­˜ç•¶å‰æ¨¡å‹ç‹€æ…‹
            del model
            tf.keras.backend.clear_session()
            gc.collect()
            model = tf.keras.models.load_model(current_model_path, compile=False)
            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='mse') # <--- éœ€è¦é‡æ–°ç·¨è­¯
            # å¯èƒ½é‚„éœ€è¦é‡æ–°å‰µå»º compiled_predict å‡½æ•¸ï¼Œå› ç‚ºå®ƒä¾è³´å…¨å±€ model
            @tf.function(input_signature=[tf.TensorSpec(shape=(1, SEQUENCE_LENGTH, num_features), dtype=tf.float32)])
            def compiled_predict(state_input):
                q_values = model(state_input, training=False)
                return q_values

            @tf.function(input_signature=[tf.TensorSpec(shape=(None, SEQUENCE_LENGTH, num_features), dtype=tf.float32)])
            def compiled_batch_predict(state_batch_input):
                """ä½¿ç”¨ tf.function ç·¨è­¯æ¨¡å‹çš„æ‰¹æ¬¡é æ¸¬"""
                # ç›´æ¥èª¿ç”¨æ¨¡å‹ï¼Œè¨­ç½® training=False
                q_values = model(state_batch_input, training=False)
                return q_values
        except Exception as load_e:
            print(f"[Error] Failed to reload model after cleanup: {load_e}")
            # é€™è£¡éœ€è¦éŒ¯èª¤è™•ç†ï¼Œæ˜¯é€€å‡ºé‚„æ˜¯å˜—è©¦ç¹¼çºŒï¼Ÿ
            raise load_e # æˆ–è€… exit()
        
# --- æœ€çµ‚ä¿å­˜æ¨¡å‹ ---
print("Training finished. Saving final model...")
model.save(MODEL_PATH)
print("Model saved.")